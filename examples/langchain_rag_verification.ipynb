{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain RAG Verification with i2i\n",
    "\n",
    "This notebook demonstrates how to use **i2i** (eye-to-eye) multi-model consensus to verify RAG pipeline outputs and detect hallucinations.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) pipelines can still hallucinate:\n",
    "- Model may ignore retrieved context and confabulate\n",
    "- Retrieval may miss relevant documents\n",
    "- Model may misinterpret or misquote sources\n",
    "\n",
    "## The Solution\n",
    "\n",
    "Use **i2i multi-model consensus** as a verification layer:\n",
    "- HIGH consensus on factual claims ‚Üí 97-100% accuracy\n",
    "- LOW/NONE consensus ‚Üí likely hallucination, needs review\n",
    "- Task-aware routing ‚Üí skip consensus for math (where it hurts!)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Build a basic RAG pipeline with LangChain + ChromaDB\n",
    "2. See hallucinations in action (confident but wrong answers)\n",
    "3. Add i2i verification to flag unreliable answers\n",
    "4. Use task-aware consensus (factual vs math vs creative)\n",
    "5. Production patterns for threshold configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Install required packages and configure API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install i2i-mcip langchain langchain-openai langchain-community chromadb wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are configured\n",
    "# i2i needs at least 2 providers for consensus\n",
    "providers_configured = []\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    providers_configured.append(\"OpenAI\")\n",
    "if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    providers_configured.append(\"Anthropic\")\n",
    "if os.getenv(\"GROQ_API_KEY\"):\n",
    "    providers_configured.append(\"Groq\")\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    providers_configured.append(\"Google\")\n",
    "\n",
    "print(f\"Configured providers: {providers_configured}\")\n",
    "if len(providers_configured) < 2:\n",
    "    print(\"\\n‚ö†Ô∏è  Warning: i2i consensus requires at least 2 providers.\")\n",
    "    print(\"Set API keys for at least 2 of: OPENAI_API_KEY, ANTHROPIC_API_KEY, GROQ_API_KEY, GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Basic RAG Pipeline\n",
    "\n",
    "We'll create a simple RAG pipeline with:\n",
    "- **ChromaDB**: Vector store for document embeddings\n",
    "- **LangChain**: Orchestration framework\n",
    "- **Sample documents**: Wikipedia articles about historical events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Sample documents (simulating retrieved Wikipedia content)\n",
    "# These contain accurate historical facts\n",
    "SAMPLE_DOCUMENTS = [\n",
    "    \"\"\"The French Revolution began in 1789 and ended in 1799. It was a period of radical political \n",
    "    and societal change in France. The revolution began with the convocation of the Estates General \n",
    "    in May 1789. The Bastille was stormed on July 14, 1789, which became a symbol of the revolution. \n",
    "    King Louis XVI was executed by guillotine on January 21, 1793.\"\"\",\n",
    "    \n",
    "    \"\"\"Albert Einstein developed the theory of special relativity in 1905 and general relativity in 1915. \n",
    "    He was awarded the Nobel Prize in Physics in 1921 for his discovery of the photoelectric effect, \n",
    "    not for relativity. Einstein was born in Ulm, Germany on March 14, 1879, and died in Princeton, \n",
    "    New Jersey on April 18, 1955.\"\"\",\n",
    "    \n",
    "    \"\"\"The Great Wall of China is approximately 21,196 kilometers (13,171 miles) long, including \n",
    "    all of its branches. Construction began in the 7th century BC and continued for over two millennia. \n",
    "    The wall is NOT visible from space with the naked eye - this is a common myth. The most famous \n",
    "    sections were built during the Ming Dynasty (1368-1644).\"\"\",\n",
    "    \n",
    "    \"\"\"Python was created by Guido van Rossum and first released in 1991. The language was named \n",
    "    after Monty Python's Flying Circus, not the snake. Python 2.0 was released in 2000 and \n",
    "    Python 3.0 in 2008. Python 2 reached end-of-life on January 1, 2020.\"\"\",\n",
    "    \n",
    "    \"\"\"The moon's average distance from Earth is about 384,400 kilometers (238,855 miles). \n",
    "    A light beam takes approximately 1.28 seconds to travel from Earth to the Moon. The moon \n",
    "    is slowly moving away from Earth at a rate of about 3.8 centimeters per year.\"\"\"\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(SAMPLE_DOCUMENTS)} sample documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.create_documents(SAMPLE_DOCUMENTS)\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag_demo\"\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "print(f\"Vector store created with {len(splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG chain\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a direct, confident answer. If you're not sure, still give your best answer.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG chain with a simple question\n",
    "response = rag_chain.invoke(\"When was the Bastille stormed?\")\n",
    "print(f\"Question: When was the Bastille stormed?\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Hallucination Problem\n",
    "\n",
    "RAG pipelines can still produce incorrect answers:\n",
    "- Questions outside the knowledge base\n",
    "- Misinterpretation of context\n",
    "- Confident confabulation\n",
    "\n",
    "Let's see some examples where the model gives confident but WRONG answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions designed to trigger hallucinations\n",
    "HALLUCINATION_QUESTIONS = [\n",
    "    # Outside knowledge base - model may confabulate\n",
    "    \"What year did Einstein fail his math exam in school?\",\n",
    "    \n",
    "    # Contradicts facts in context (Great Wall myth)\n",
    "    \"Can you see the Great Wall of China from space?\",\n",
    "    \n",
    "    # Not in context - might confabulate\n",
    "    \"Who was the architect of the Eiffel Tower?\",\n",
    "    \n",
    "    # Partially related but wrong inference likely\n",
    "    \"What animal is Python named after?\",\n",
    "    \n",
    "    # Math question embedded in factual context\n",
    "    \"If light takes 1.28 seconds to reach the moon, how long for a round trip?\"\n",
    "]\n",
    "\n",
    "print(\"Testing questions that may cause hallucinations...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q in HALLUCINATION_QUESTIONS:\n",
    "    response = rag_chain.invoke(q)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Notice how the model:\n",
    "1. **Einstein's math exam**: May confidently claim he failed (common myth, actually FALSE)\n",
    "2. **Great Wall from space**: Our context explicitly says this is a myth, but model might still say yes\n",
    "3. **Eiffel Tower architect**: Not in our docs - model may confabulate an answer\n",
    "4. **Python naming**: Context says Monty Python, but model might say snake\n",
    "5. **Math question**: Simple calculation, but consensus would hurt here!\n",
    "\n",
    "**The problem**: The model gives confident answers with no indication of reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add i2i Verification\n",
    "\n",
    "Now let's add i2i multi-model consensus to verify RAG outputs.\n",
    "\n",
    "Key insight from evaluation (400 questions, 4 models):\n",
    "- **HIGH consensus (‚â•85% agreement)**: 97-100% accuracy\n",
    "- **LOW/NONE consensus**: Likely hallucination\n",
    "- **Consensus on math/reasoning**: -35% accuracy (DON'T use it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from i2i import (\n",
    "    AICP,\n",
    "    ConsensusLevel,\n",
    "    recommend_consensus,\n",
    "    is_consensus_appropriate,\n",
    "    get_confidence_calibration,\n",
    ")\n",
    "\n",
    "# Initialize i2i protocol\n",
    "protocol = AICP()\n",
    "\n",
    "print(\"Available providers:\", protocol.list_configured_providers())\n",
    "print(\"Available models:\", protocol.list_available_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I2IVerifiedRAG:\n",
    "    \"\"\"\n",
    "    RAG chain wrapper with i2i verification.\n",
    "    \n",
    "    Adds multi-model consensus verification to RAG outputs,\n",
    "    with task-aware routing to avoid hurting math/reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_chain, protocol: AICP, confidence_threshold: float = 0.7):\n",
    "        self.rag_chain = rag_chain\n",
    "        self.protocol = protocol\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "    \n",
    "    async def query(self, question: str) -> dict:\n",
    "        \"\"\"\n",
    "        Query the RAG chain with i2i verification.\n",
    "        \n",
    "        Returns:\n",
    "            dict with:\n",
    "                - answer: The RAG response\n",
    "                - verified: Whether answer passed consensus check\n",
    "                - confidence: Calibrated confidence score\n",
    "                - consensus_level: HIGH/MEDIUM/LOW/NONE\n",
    "                - task_appropriate: Whether consensus was appropriate for this task\n",
    "                - warning: Any warnings about the answer\n",
    "        \"\"\"\n",
    "        # Step 1: Get RAG response\n",
    "        rag_answer = self.rag_chain.invoke(question)\n",
    "        \n",
    "        # Step 2: Check if consensus is appropriate for this task\n",
    "        recommendation = recommend_consensus(question)\n",
    "        \n",
    "        if not recommendation.should_use_consensus:\n",
    "            # For math/reasoning/creative, skip consensus\n",
    "            return {\n",
    "                \"answer\": rag_answer,\n",
    "                \"verified\": None,  # Not applicable\n",
    "                \"confidence\": None,\n",
    "                \"consensus_level\": None,\n",
    "                \"task_appropriate\": False,\n",
    "                \"task_category\": recommendation.task_category.value,\n",
    "                \"warning\": f\"Consensus skipped: {recommendation.reason}\",\n",
    "                \"suggested_approach\": recommendation.suggested_approach\n",
    "            }\n",
    "        \n",
    "        # Step 3: Verify with multi-model consensus\n",
    "        verification_query = f\"\"\"Verify this answer is factually correct:\n",
    "\n",
    "Question: {question}\n",
    "Answer: {rag_answer}\n",
    "\n",
    "Is this answer accurate? Respond with TRUE or FALSE and brief reasoning.\"\"\"\n",
    "        \n",
    "        result = await self.protocol.consensus_query(\n",
    "            verification_query,\n",
    "            task_category=\"verification\"\n",
    "        )\n",
    "        \n",
    "        # Step 4: Calculate calibrated confidence\n",
    "        confidence = get_confidence_calibration(result.consensus_level.value)\n",
    "        verified = confidence >= self.confidence_threshold\n",
    "        \n",
    "        warning = None\n",
    "        if result.consensus_level in [ConsensusLevel.LOW, ConsensusLevel.NONE]:\n",
    "            warning = \"‚ö†Ô∏è LOW CONFIDENCE: Models disagree. Answer may be unreliable.\"\n",
    "        elif result.consensus_level == ConsensusLevel.CONTRADICTORY:\n",
    "            warning = \"‚ö†Ô∏è CONTRADICTION: Models actively disagree. Do not trust this answer.\"\n",
    "        \n",
    "        return {\n",
    "            \"answer\": rag_answer,\n",
    "            \"verified\": verified,\n",
    "            \"confidence\": confidence,\n",
    "            \"consensus_level\": result.consensus_level.value,\n",
    "            \"task_appropriate\": True,\n",
    "            \"task_category\": result.task_category,\n",
    "            \"warning\": warning,\n",
    "            \"models_consulted\": result.models_queried\n",
    "        }\n",
    "\n",
    "# Create verified RAG instance\n",
    "verified_rag = I2IVerifiedRAG(rag_chain, protocol)\n",
    "print(\"I2IVerifiedRAG ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to run async in notebook\n",
    "async def test_verified_rag(questions: list[str]):\n",
    "    \"\"\"Test verified RAG on a list of questions.\"\"\"\n",
    "    results = []\n",
    "    for q in questions:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Q: {q}\")\n",
    "        \n",
    "        result = await verified_rag.query(q)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\nA: {result['answer']}\")\n",
    "        print(f\"\\nüìä Verification Results:\")\n",
    "        print(f\"   Task Category: {result['task_category']}\")\n",
    "        print(f\"   Consensus Appropriate: {result['task_appropriate']}\")\n",
    "        \n",
    "        if result['task_appropriate']:\n",
    "            print(f\"   Consensus Level: {result['consensus_level']}\")\n",
    "            print(f\"   Confidence: {result['confidence']:.0%}\")\n",
    "            print(f\"   Verified: {'‚úÖ' if result['verified'] else '‚ùå'} {result['verified']}\")\n",
    "            if result.get('models_consulted'):\n",
    "                print(f\"   Models: {', '.join(result['models_consulted'])}\")\n",
    "        \n",
    "        if result['warning']:\n",
    "            print(f\"\\n   {result['warning']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on our hallucination-prone questions\n",
    "results = await test_verified_rag(HALLUCINATION_QUESTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "1. **Factual questions** ‚Üí Consensus used, confidence score provided\n",
    "2. **Math question** (round trip light) ‚Üí Consensus SKIPPED (would hurt accuracy)\n",
    "3. **Low confidence** ‚Üí Warning displayed, answer flagged as unreliable\n",
    "\n",
    "This is the power of **task-aware consensus**: knowing WHEN to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Task-Aware Behavior Demo\n",
    "\n",
    "Let's demonstrate how i2i handles different task types differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate task classification\n",
    "DEMO_QUESTIONS = [\n",
    "    # Factual - USE consensus (HIGH = 97% accuracy)\n",
    "    (\"What year was Python first released?\", \"factual\"),\n",
    "    (\"Who was executed during the French Revolution?\", \"factual\"),\n",
    "    \n",
    "    # Verification - USE consensus (+6% hallucination detection)\n",
    "    (\"Is it true that the Great Wall is visible from space?\", \"verification\"),\n",
    "    (\"True or false: Einstein won the Nobel Prize for relativity\", \"verification\"),\n",
    "    \n",
    "    # Math/Reasoning - DON'T use consensus (-35% degradation!)\n",
    "    (\"Calculate: if the moon moves 3.8cm/year, how far in 100 years?\", \"reasoning\"),\n",
    "    (\"If Python 3.0 came out in 2008, how old is it now?\", \"reasoning\"),\n",
    "    \n",
    "    # Creative - DON'T use consensus (flattens diversity)\n",
    "    (\"Write a haiku about the French Revolution\", \"creative\"),\n",
    "]\n",
    "\n",
    "print(\"Task Classification Demo\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for question, expected in DEMO_QUESTIONS:\n",
    "    rec = recommend_consensus(question)\n",
    "    status = \"‚úÖ\" if rec.should_use_consensus else \"‚ùå\"\n",
    "    \n",
    "    print(f\"\\n{status} {question[:55]}...\")\n",
    "    print(f\"   Detected: {rec.task_category.value} (expected: {expected})\")\n",
    "    print(f\"   Use consensus: {rec.should_use_consensus}\")\n",
    "    if not rec.should_use_consensus:\n",
    "        print(f\"   Instead: {rec.suggested_approach[:60]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confidence calibration\n",
    "print(\"\\nConfidence Calibration (from evaluation data)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nBased on 400 questions across 5 benchmarks:\\n\")\n",
    "\n",
    "calibration_data = [\n",
    "    (\"HIGH (‚â•85% agreement)\", \"high\", \"Trust the answer\"),\n",
    "    (\"MEDIUM (60-84%)\", \"medium\", \"Probably correct\"),\n",
    "    (\"LOW (30-59%)\", \"low\", \"Use with caution\"),\n",
    "    (\"NONE (<30%)\", \"none\", \"Likely hallucination\"),\n",
    "    (\"CONTRADICTORY\", \"contradictory\", \"Models disagree - investigate\"),\n",
    "]\n",
    "\n",
    "for name, level, meaning in calibration_data:\n",
    "    conf = get_confidence_calibration(level)\n",
    "    print(f\"  {name:25} ‚Üí {conf:.0%} confidence ({meaning})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Patterns\n",
    "\n",
    "Best practices for deploying i2i verification in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production configuration example\n",
    "PRODUCTION_CONFIG = {\n",
    "    # Confidence thresholds\n",
    "    \"high_confidence_threshold\": 0.90,  # Accept without review\n",
    "    \"low_confidence_threshold\": 0.60,   # Flag for human review\n",
    "    \"reject_threshold\": 0.50,           # Reject outright\n",
    "    \n",
    "    # Model selection for consensus\n",
    "    \"consensus_models\": [\n",
    "        \"gpt-4o\",\n",
    "        \"claude-3-5-sonnet-latest\",\n",
    "        \"gemini-1.5-pro\",\n",
    "    ],\n",
    "    \n",
    "    # Cost management\n",
    "    \"max_models_per_query\": 3,\n",
    "    \"use_cheaper_models_for_obvious\": True,\n",
    "    \n",
    "    # Logging\n",
    "    \"log_all_queries\": True,\n",
    "    \"log_low_confidence\": True,\n",
    "}\n",
    "\n",
    "print(\"Production Configuration:\")\n",
    "for key, value in PRODUCTION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionVerifiedRAG:\n",
    "    \"\"\"\n",
    "    Production-ready RAG with i2i verification.\n",
    "    \n",
    "    Features:\n",
    "    - Configurable confidence thresholds\n",
    "    - Structured logging\n",
    "    - Error handling\n",
    "    - Cost tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_chain, config: dict = None):\n",
    "        self.rag_chain = rag_chain\n",
    "        self.config = config or PRODUCTION_CONFIG\n",
    "        self.protocol = AICP()\n",
    "        self.query_log = []\n",
    "    \n",
    "    async def query(self, question: str) -> dict:\n",
    "        \"\"\"Query with production-grade verification.\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Get RAG response\n",
    "            rag_answer = self.rag_chain.invoke(question)\n",
    "            \n",
    "            # Check task type\n",
    "            rec = recommend_consensus(question)\n",
    "            \n",
    "            if not rec.should_use_consensus:\n",
    "                result = self._build_result(\n",
    "                    question=question,\n",
    "                    answer=rag_answer,\n",
    "                    verified=None,\n",
    "                    confidence=None,\n",
    "                    consensus_level=None,\n",
    "                    task_category=rec.task_category.value,\n",
    "                    status=\"skipped\",\n",
    "                    reason=rec.reason,\n",
    "                    latency_ms=(time.time() - start_time) * 1000\n",
    "                )\n",
    "            else:\n",
    "                # Run consensus verification\n",
    "                consensus_result = await self.protocol.consensus_query(\n",
    "                    f\"Verify: {question} -> {rag_answer}\",\n",
    "                    task_category=\"verification\"\n",
    "                )\n",
    "                \n",
    "                confidence = get_confidence_calibration(consensus_result.consensus_level.value)\n",
    "                status = self._determine_status(confidence)\n",
    "                \n",
    "                result = self._build_result(\n",
    "                    question=question,\n",
    "                    answer=rag_answer,\n",
    "                    verified=confidence >= self.config[\"low_confidence_threshold\"],\n",
    "                    confidence=confidence,\n",
    "                    consensus_level=consensus_result.consensus_level.value,\n",
    "                    task_category=consensus_result.task_category,\n",
    "                    status=status,\n",
    "                    models_used=consensus_result.models_queried,\n",
    "                    latency_ms=(time.time() - start_time) * 1000\n",
    "                )\n",
    "            \n",
    "            # Log query\n",
    "            self._log_query(result)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self._build_result(\n",
    "                question=question,\n",
    "                answer=None,\n",
    "                verified=False,\n",
    "                confidence=0,\n",
    "                status=\"error\",\n",
    "                error=str(e),\n",
    "                latency_ms=(time.time() - start_time) * 1000\n",
    "            )\n",
    "    \n",
    "    def _determine_status(self, confidence: float) -> str:\n",
    "        if confidence >= self.config[\"high_confidence_threshold\"]:\n",
    "            return \"accepted\"\n",
    "        elif confidence >= self.config[\"low_confidence_threshold\"]:\n",
    "            return \"review\"\n",
    "        elif confidence >= self.config[\"reject_threshold\"]:\n",
    "            return \"low_confidence\"\n",
    "        else:\n",
    "            return \"rejected\"\n",
    "    \n",
    "    def _build_result(self, **kwargs) -> dict:\n",
    "        return {\n",
    "            \"question\": kwargs.get(\"question\"),\n",
    "            \"answer\": kwargs.get(\"answer\"),\n",
    "            \"verified\": kwargs.get(\"verified\"),\n",
    "            \"confidence\": kwargs.get(\"confidence\"),\n",
    "            \"consensus_level\": kwargs.get(\"consensus_level\"),\n",
    "            \"task_category\": kwargs.get(\"task_category\"),\n",
    "            \"status\": kwargs.get(\"status\"),\n",
    "            \"reason\": kwargs.get(\"reason\"),\n",
    "            \"models_used\": kwargs.get(\"models_used\", []),\n",
    "            \"latency_ms\": kwargs.get(\"latency_ms\"),\n",
    "            \"error\": kwargs.get(\"error\"),\n",
    "        }\n",
    "    \n",
    "    def _log_query(self, result: dict):\n",
    "        if self.config.get(\"log_all_queries\"):\n",
    "            self.query_log.append(result)\n",
    "        elif self.config.get(\"log_low_confidence\") and result[\"status\"] in [\"low_confidence\", \"rejected\"]:\n",
    "            self.query_log.append(result)\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get statistics from query log.\"\"\"\n",
    "        if not self.query_log:\n",
    "            return {\"total_queries\": 0}\n",
    "        \n",
    "        statuses = [q[\"status\"] for q in self.query_log]\n",
    "        latencies = [q[\"latency_ms\"] for q in self.query_log if q.get(\"latency_ms\")]\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": len(self.query_log),\n",
    "            \"accepted\": statuses.count(\"accepted\"),\n",
    "            \"review\": statuses.count(\"review\"),\n",
    "            \"low_confidence\": statuses.count(\"low_confidence\"),\n",
    "            \"rejected\": statuses.count(\"rejected\"),\n",
    "            \"skipped\": statuses.count(\"skipped\"),\n",
    "            \"errors\": statuses.count(\"error\"),\n",
    "            \"avg_latency_ms\": sum(latencies) / len(latencies) if latencies else 0,\n",
    "        }\n",
    "\n",
    "# Create production instance\n",
    "prod_rag = ProductionVerifiedRAG(rag_chain)\n",
    "print(\"ProductionVerifiedRAG ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run production demo\n",
    "async def production_demo():\n",
    "    questions = [\n",
    "        \"When was Python first released?\",\n",
    "        \"Is the Great Wall visible from space?\",\n",
    "        \"Calculate 384400 / 1.28\",\n",
    "        \"Who invented Python?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Production Demo\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for q in questions:\n",
    "        result = await prod_rag.query(q)\n",
    "        \n",
    "        status_emoji = {\n",
    "            \"accepted\": \"‚úÖ\",\n",
    "            \"review\": \"‚ö†Ô∏è\",\n",
    "            \"low_confidence\": \"‚ùå\",\n",
    "            \"rejected\": \"üö´\",\n",
    "            \"skipped\": \"‚è≠Ô∏è\",\n",
    "            \"error\": \"üí•\",\n",
    "        }\n",
    "        \n",
    "        emoji = status_emoji.get(result[\"status\"], \"‚ùì\")\n",
    "        \n",
    "        print(f\"\\n{emoji} {result['status'].upper()}\")\n",
    "        print(f\"   Q: {q}\")\n",
    "        print(f\"   A: {result['answer'][:100]}...\" if result['answer'] else \"   A: [no answer]\")\n",
    "        if result['confidence']:\n",
    "            print(f\"   Confidence: {result['confidence']:.0%}\")\n",
    "        print(f\"   Latency: {result['latency_ms']:.0f}ms\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\nQuery Statistics:\")\n",
    "    stats = prod_rag.get_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "await production_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **RAG still hallucinates** - Retrieval alone doesn't guarantee accuracy\n",
    "\n",
    "2. **Multi-model consensus provides calibrated confidence**:\n",
    "   - HIGH consensus ‚Üí 97-100% accuracy\n",
    "   - LOW/NONE consensus ‚Üí Flag for review\n",
    "\n",
    "3. **Task-awareness is critical**:\n",
    "   - ‚úÖ Use consensus for: factual, verification, commonsense\n",
    "   - ‚ùå Skip consensus for: math, reasoning, creative\n",
    "\n",
    "4. **Production deployment**:\n",
    "   - Configure confidence thresholds\n",
    "   - Log low-confidence queries\n",
    "   - Route to human review when uncertain\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Check out `examples/task_aware_consensus.py` for more task classification examples\n",
    "- See `demo.py verify` for standalone claim verification\n",
    "- Review the [i2i documentation](https://github.com/unit221b/i2i) for full API reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "vectorstore.delete_collection()\n",
    "print(\"Cleaned up vector store\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
