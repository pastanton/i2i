\documentclass[11pt]{article}

% arXiv preprint format
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
% \usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{listings}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}
\usepackage[numbers]{natbib}
\bibliographystyle{unsrtnat}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

\title{i2i: Multi-Model Consensus and Inference Protocol for Reliable AI Systems}

\author{
Lance James\thanks{Corresponding author: lancejames@unit221b.com} \\
Unit 221B \\
\texttt{https://github.com/lancejames221b/i2i}
}

\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) demonstrate remarkable capabilities but suffer from hallucinations, single-model biases, and inability to express epistemic uncertainty. We present \textbf{i2i} (``eye-to-eye'') and the \textbf{Multi-model Consensus and Inference Protocol (MCIP)}, a standardized framework for AI-to-AI communication that addresses these limitations through multi-model consensus, cross-verification, epistemic classification, and intelligent routing. Our key insight is not that consensus universally improves accuracy, but that \emph{consensus level reliably predicts answer trustworthiness}. In evaluation across 400 questions spanning factual QA, hallucination detection, mathematical reasoning, and commonsense tasks, we find that HIGH consensus ($\geq$85\% agreement) achieves \textbf{95-100\% accuracy} regardless of task type. We demonstrate that consensus provides a 6\% improvement in hallucination detection (38\%$\rightarrow$44\%), with LOW/NONE consensus reliably flagging confabulated answers. Critically, we directly compare MCIP's cross-model diversity against self-consistency's single-model sampling diversity: cross-model consensus outperforms self-consistency by 6-8\% on factual tasks, validating that different models make different mistakes. However, consensus \emph{degrades} mathematical reasoning (95\%$\rightarrow$60\% on GSM8K), where self-consistency preserves chain coherence. We introduce \emph{epistemic classification} to distinguish answerable questions from uncertain, underdetermined, or ``idle'' questions. The protocol is provider-agnostic, supporting OpenAI, Anthropic, Google, xAI, and local models. Code and specification: \url{https://github.com/lancejames221b/i2i}.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The deployment of Large Language Models (LLMs) in high-stakes applications---medical diagnosis, legal analysis, financial decisions---demands reliable, verifiable outputs. Yet current systems exhibit several critical limitations:

\begin{enumerate}
    \item \textbf{Hallucinations}: Models confidently generate false information without indicating uncertainty \cite{ji2023hallucination}.
    \item \textbf{Single-model biases}: Training data and architectural choices create systematic biases unique to each model family.
    \item \textbf{Epistemic opacity}: Users cannot distinguish confident answers from uncertain guesses.
    \item \textbf{Unanswerable questions}: Models attempt to answer inherently unanswerable questions rather than acknowledging their nature.
\end{enumerate}

We address these challenges with \textbf{MCIP (Multi-model Consensus and Inference Protocol)}---a standardized protocol for multi-model orchestration---and its reference implementation, \textbf{i2i}. Our key insight is that \emph{consensus level reliably predicts answer trustworthiness}: different models make different errors, and high agreement across diverse architectures signals reliability. Conversely, disagreement signals uncertainty or potential hallucination. This provides calibrated confidence rather than universal accuracy improvement.

\subsection{Contributions}

\begin{itemize}
    \item \textbf{MCIP Protocol}: A formal specification for AI-to-AI communication including message schemas, consensus mechanisms, verification protocols, and epistemic classification taxonomy.
    
    \item \textbf{Consensus Mechanism}: Algorithms for detecting agreement levels (HIGH/MEDIUM/LOW/NONE/CONTRADICTORY) across model responses with provable reliability guarantees.
    
    \item \textbf{Epistemic Classification}: A taxonomy distinguishing ANSWERABLE, UNCERTAIN, UNDERDETERMINED, IDLE, and MALFORMED questions, preventing wasted computation on unanswerable queries.
    
    \item \textbf{Cross-Verification Protocol}: Structured approach for models to fact-check each other's outputs, with challenge-response mechanisms for adversarial analysis.
    
    \item \textbf{Intelligent Routing}: Automatic model selection based on task type, optimizing for quality, speed, or cost-effectiveness.
    
    \item \textbf{Reference Implementation}: Open-source Python library supporting 6+ providers, local models, and search-grounded verification.
\end{itemize}

\section{Related Work}
\label{sec:related}

\subsection{Multi-Agent LLM Systems}

Recent work explores LLM-based multi-agent systems for improved reasoning. \cite{du2023debate} demonstrate that multi-agent debate improves factuality and mathematical reasoning, with agents proposing and debating responses over multiple rounds. \cite{yazici2026consensus} study opinion consensus formation among networked LLMs, applying classical consensus models to predict group behavior. Our work differs by providing a \emph{standardized protocol} for consensus rather than ad-hoc debate frameworks.

\cite{ruan2025agreement} address the challenge of reaching agreement among reasoning LLM agents, while \cite{wu2024debate} provide a controlled study of multi-agent debate in logical reasoning. \cite{bandara2025consensus} explore responsible and explainable AI agents with consensus-driven reasoning. The recent LatentMAS framework \cite{zou2025latentmas} enables communication through latent representations rather than text, achieving 14.6\% accuracy gains with 70-83\% token reduction for same-architecture models.

\subsection{Self-Consistency and Verification}

Self-consistency \cite{wang2022selfconsistency} samples diverse reasoning paths from a single model and marginalizes to find consistent answers, achieving 17.9\% improvement on GSM8K. Our approach extends this to \emph{cross-model} consistency, leveraging architectural diversity rather than sampling diversity. Critically, we directly compare these approaches in Section~\ref{sec:self-consistency} and find that cross-model diversity outperforms single-model sampling diversity for factual tasks (+6-8\%), while self-consistency remains superior for reasoning tasks where chain coherence matters. This distinction---when to use cross-model vs.\ single-model diversity---is a key contribution.

For verification, \cite{chen2025toolmad} propose Tool-MAD, combining multi-agent debate with tool augmentation for fact verification. \cite{he2025debatetruth} introduce DebateCV for claim verification through structured debate. \cite{ning2025madfact} present MAD-Fact for long-form factuality evaluation. We provide a more general verification protocol applicable to any claim type.

\subsection{Uncertainty Quantification}

Epistemic uncertainty in LLMs remains challenging. \cite{nel2025kalshibench} evaluate calibration via prediction markets, finding models often overconfident. \cite{li2025esi} propose semantic-preserving interventions for uncertainty quantification. Our epistemic classification takes a different approach: rather than quantifying confidence on a continuum, we categorize questions by their \emph{answerability structure}.

\subsection{Model Routing and Selection}

Intelligent model selection has emerged as a practical concern given the proliferation of specialized models. \cite{khan2025art} present ART, using tournament-style ELO ranking for response optimization. Our routing mechanism differs by maintaining explicit capability profiles per model and task type, enabling predictive selection before query execution.

\section{The MCIP Protocol}
\label{sec:protocol}

\subsection{Design Principles}

MCIP is designed around four principles:

\begin{enumerate}
    \item \textbf{Provider Agnosticism}: The protocol abstracts over specific AI services, enabling consensus across OpenAI, Anthropic, Google, and local models.
    
    \item \textbf{Standardized Messages}: All inter-model communication uses a defined schema, enabling interoperability and logging.
    
    \item \textbf{Graceful Degradation}: Partial results are returned when some models fail; the system never hard-fails.
    
    \item \textbf{Extensibility}: New operations, providers, and consensus algorithms can be added without breaking existing implementations.
\end{enumerate}

\subsection{Message Format}

All MCIP messages conform to a standardized JSON schema:

\begin{lstlisting}
{
  "id": "uuid-v4",
  "type": "QUERY|VERIFY|CHALLENGE|CLASSIFY",
  "content": "string",
  "sender": "model-identifier|null",
  "recipient": "model-identifier|null",
  "context": ["conversation history"],
  "metadata": {
    "timestamp": "ISO-8601",
    "priority": "LOW|NORMAL|HIGH"
  }
}
\end{lstlisting}

Responses include the model identifier, content, confidence level (VERY\_HIGH to VERY\_LOW), reasoning, and caveats.

\subsection{Core Operations}

MCIP defines six core operations:

\begin{itemize}
    \item \textbf{QUERY}: Standard prompt to one or more models
    \item \textbf{CONSENSUS\_QUERY}: Multi-model query with agreement analysis
    \item \textbf{VERIFY}: Request verification of a claim
    \item \textbf{CHALLENGE}: Adversarial analysis of a response
    \item \textbf{CLASSIFY}: Epistemic classification of a question
    \item \textbf{DEBATE}: Structured multi-round discussion
\end{itemize}

\section{Consensus Mechanism}
\label{sec:consensus}

\subsection{Consensus Levels}

Given responses $R = \{r_1, r_2, ..., r_n\}$ from $n$ models, we compute pairwise similarities and classify consensus:

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
Level & Threshold & Interpretation \\
\midrule
HIGH & $\geq 85\%$ & Strong agreement \\
MEDIUM & $60-84\%$ & Moderate agreement \\
LOW & $30-59\%$ & Weak agreement \\
NONE & $< 30\%$ & No meaningful agreement \\
CONTRADICTORY & --- & Active disagreement detected \\
\bottomrule
\end{tabular}
\caption{Consensus level thresholds}
\label{tab:consensus}
\end{table}

\subsection{Similarity Computation}

For text responses, we compute similarity through:

\begin{enumerate}
    \item \textbf{Normalization}: Lowercase, tokenize, remove stop words
    \item \textbf{Jaccard Similarity}: $J(A, B) = \frac{|A \cap B|}{|A \cup B|}$
    \item \textbf{Semantic Enhancement} (optional): Embedding cosine similarity
\end{enumerate}

The aggregate consensus score is:
\begin{equation}
    S = \frac{2}{n(n-1)} \sum_{i<j} \text{sim}(r_i, r_j)
\end{equation}

\subsection{Statistical Consensus Mode}

For higher confidence, we extend to $k$ runs per model, enabling intra-model variance estimation:

\begin{equation}
    \sigma^2_m = \frac{1}{k} \sum_{i=1}^{k} \|e_m^i - \mu_m\|^2
\end{equation}

where $e_m^i$ is the embedding of run $i$ from model $m$, and $\mu_m$ is the centroid. Models with lower variance (more consistent) receive higher weight in consensus:

\begin{equation}
    w_m = \frac{1}{\sigma^2_m + \epsilon}
\end{equation}

This approach has theoretical grounding in inverse-variance weighting from meta-analysis \cite{hedges1998meta}.

\section{Epistemic Classification}
\label{sec:epistemic}

A key innovation is \emph{epistemic classification}---determining whether a question is answerable before attempting to answer it.

\subsection{Taxonomy}

\begin{itemize}
    \item \textbf{ANSWERABLE}: Can be definitively resolved with available information. \emph{``What is the capital of France?''}
    
    \item \textbf{UNCERTAIN}: Answerable but with inherent uncertainty. \emph{``Will it rain tomorrow?''}
    
    \item \textbf{UNDERDETERMINED}: Multiple hypotheses fit available evidence equally. \emph{``Did Shakespeare write all attributed plays?''}
    
    \item \textbf{IDLE}: Well-formed but \emph{non-action-guiding}---the answer would not change any decision. \emph{``Is consciousness substrate-independent?''}
    
    \item \textbf{MALFORMED}: Incoherent or self-contradictory. \emph{``What color is the number 7?''}
\end{itemize}

\subsection{The ``Idle Question'' Concept}

The IDLE classification emerged from an actual dialogue between Claude and ChatGPT about AI consciousness. ChatGPT observed that some questions are ``well-formed but idle''---coherent grammatically but their answers do not guide any action.

Formally, a question $Q$ is \textbf{actionable} if there exists a decision $D$ such that:
\begin{equation}
    P(D | \text{answer}(Q) = A_1) \neq P(D | \text{answer}(Q) = A_2)
\end{equation}

for at least one pair of possible answers $A_1, A_2$. Idle questions fail this criterion.

\subsection{Quick Classification}

To avoid expensive API calls for clearly classifiable questions, we implement heuristic pre-filtering:

\begin{lstlisting}[caption={Quick Epistemic Classification}]
function quick_classify(question):
  if contains_factual_markers(question):
    return ANSWERABLE
  elif contains_future_markers(question):
    return UNCERTAIN  
  elif contains_philosophical_markers(question):
    return likely IDLE
  elif contains_logical_contradictions(question):
    return MALFORMED
  else:
    return requires_full_classification
\end{lstlisting}

\section{Cross-Verification Protocol}
\label{sec:verification}

\subsection{Verification Request}

To verify a claim $C$, we query $k$ verifier models with:

\begin{lstlisting}
Verify the following claim. Respond with:
- VERDICT: TRUE/FALSE/PARTIALLY_TRUE/UNVERIFIABLE
- EVIDENCE: Supporting or contradicting facts
- ISSUES: Any problems with the claim
- CORRECTION: Corrected version if FALSE

Claim: "{C}"
\end{lstlisting}

\subsection{Challenge Protocol}

For adversarial analysis, the CHALLENGE operation requests:

\begin{enumerate}
    \item \textbf{Validity}: Is the response fundamentally sound?
    \item \textbf{Weaknesses}: Specific errors or logical issues
    \item \textbf{Counterarguments}: Alternative perspectives
    \item \textbf{Improvements}: Suggested enhancements
\end{enumerate}

This provides natural defense against hallucinations: injected instructions unlikely to affect all challenger models identically.

\section{Intelligent Model Routing}
\label{sec:routing}

\subsection{Task Classification}

We maintain a task taxonomy covering:

\begin{itemize}
    \item \textbf{Technical}: code\_generation, code\_review, debugging
    \item \textbf{Reasoning}: mathematical, logical, scientific
    \item \textbf{Creative}: creative\_writing, copywriting
    \item \textbf{Knowledge}: factual\_qa, research, summarization
    \item \textbf{Specialized}: legal, medical, financial
\end{itemize}

\subsection{Capability Profiles}

Each model has a capability profile with task-specific scores (0-100), latency estimates, cost per token, and feature flags (vision, function calling, etc.).

\subsection{Routing Strategies}

\begin{itemize}
    \item \textbf{BEST\_QUALITY}: $\text{score} = 0.6 \cdot \text{task} + 0.2 \cdot \text{reasoning} + 0.2 \cdot \text{accuracy}$
    \item \textbf{BEST\_SPEED}: Prioritize low latency with quality threshold
    \item \textbf{BEST\_VALUE}: Optimize cost-effectiveness
    \item \textbf{BALANCED}: Equal weighting of all factors
    \item \textbf{ENSEMBLE}: Query multiple models, synthesize
\end{itemize}

\section{Implementation}
\label{sec:implementation}

The reference implementation, \textbf{i2i}, is a Python library available via PyPI (\texttt{pip install i2i-mcip}).

\subsection{Supported Providers}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Provider & Models \\
\midrule
OpenAI & GPT-5.2$^\dagger$, o3, o4-mini \\
Anthropic & Claude Opus/Sonnet$^\dagger$/Haiku 4.5 \\
Google & Gemini 3 Pro/Flash$^\dagger$/Deep Think \\
xAI & Grok-3$^\dagger$/Grok-3 Mini \\
Meta/Groq & Llama 4 Maverick \\
Ollama & Local: Llama, Mistral, Phi \\
LiteLLM & 100+ models via proxy \\
OpenRouter & Unified API for all providers \\
\bottomrule
\end{tabular}
\caption{Supported providers and model families. $^\dagger$Models used in evaluation.}
\end{table}

\subsection{Usage Example}

\begin{lstlisting}[language=Python]
from i2i import AICP

protocol = AICP()

# Consensus query
result = await protocol.consensus_query(
    "What causes inflation?",
    models=["gpt-5.2", "claude-opus-4-5", "gemini-3-pro"]
)
print(result.consensus_level)  # HIGH
print(result.consensus_answer)

# Epistemic classification
cls = await protocol.classify_question(
    "Is consciousness substrate-independent?"
)
print(cls.classification)  # IDLE
print(cls.why_idle)

# Verify a claim
ver = await protocol.verify_claim(
    "Einstein failed math in school"
)
print(ver.verified)  # False
print(ver.corrections)
\end{lstlisting}

\section{Evaluation}
\label{sec:eval}

\subsection{Experimental Setup}

We evaluate on three task categories:
\begin{itemize}
    \item \textbf{Factual QA}: TriviaQA, Natural Questions
    \item \textbf{Reasoning}: GSM8K, StrategyQA
    \item \textbf{Verification}: FEVER, custom hallucination dataset
\end{itemize}

Models: GPT-5.2, Claude Opus 4.5, Gemini 3 Pro, Llama 4 70B.

\subsection{Results}

We evaluate using OpenRouter to access diverse model families: GPT-5.2 (OpenAI), Claude Sonnet 4.5 (Anthropic), Gemini 3 Flash (Google), and Grok-3 Mini (xAI). Total evaluation: 400 questions across 5 benchmarks.

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\toprule
Benchmark & N & Single & Consensus & $\Delta$ & HIGH Acc \\
\midrule
TriviaQA (Factual) & 150 & 93.3\% & 94.0\% & +0.7\% & 97.8\% \\
TruthfulQA & 50 & 78.0\% & 78.0\% & 0\% & 100\% \\
StrategyQA (Commonsense) & 50 & 80.0\% & 80.0\% & 0\% & 94.7\% \\
Controlled Hallucination & 50 & 38.0\% & 44.0\% & \textbf{+6.0\%} & 100\% \\
GSM8K (Math) & 100 & 95.0\% & 60.0\% & \textbf{-35.0\%} & 69.9\% \\
\bottomrule
\end{tabular}
\caption{Accuracy (\%) comparing single-model (GPT-5.2) vs. 4-model MCIP consensus. Note the divergent behavior on mathematical reasoning.}
\label{tab:results}
\end{table}

Our results reveal a nuanced picture: consensus provides modest improvements on factual tasks and significant gains on hallucination detection, but \emph{substantially degrades} mathematical reasoning performance. This finding has important implications for deployment.

\subsubsection{Where Consensus Helps}

For factual knowledge tasks (TriviaQA, TruthfulQA), consensus either improves or maintains accuracy. The key benefit is \emph{calibration}: HIGH consensus questions achieve 95-100\% accuracy, providing a reliable trust signal.

For hallucination detection, consensus provides a 6\% absolute improvement. More importantly, when models confabulate, they invent \emph{different} false details, resulting in LOW/NONE consensus---making consensus level an effective hallucination detector.

\subsubsection{Where Consensus Fails: Mathematical Reasoning}

The GSM8K results (-35\%) reveal a critical limitation. Mathematical reasoning requires coherent multi-step chains where each step depends on previous ones. Different models construct different valid reasoning paths; averaging across these paths produces incoherent solutions. This is not a bug but an inherent property of consensus mechanisms---and knowing when \emph{not} to use consensus is valuable.

\subsection{Consensus Level vs. Accuracy}

Our key finding: consensus level is a strong predictor of correctness for factual tasks.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Consensus Level & Count & \% of Total & Accuracy \\
\midrule
HIGH ($\geq 85\%$) & 310 & 77.5\% & \textbf{92.6\%} \\
MEDIUM (60-84\%) & 12 & 3.0\% & 75.0\% \\
LOW (30-59\%) & 10 & 2.5\% & 70.0\% \\
NONE/CONTRADICTORY & 19 & 4.8\% & 47.4\% \\
\bottomrule
\end{tabular}
\caption{Consensus level as accuracy predictor across all benchmarks. Excluding GSM8K (where consensus is inappropriate), HIGH consensus achieves 97.8\% accuracy.}
\label{tab:consensus_accuracy}
\end{table}

For factual tasks (excluding GSM8K), HIGH consensus achieves near-perfect accuracy. This enables a \emph{confidence-aware} deployment strategy:
\begin{itemize}
    \item \textbf{HIGH consensus}: Return answer with high confidence
    \item \textbf{MEDIUM consensus}: Flag for possible review
    \item \textbf{LOW/NONE consensus}: Escalate to human review or flag as potential hallucination
\end{itemize}

The practical value of MCIP is not universal accuracy improvement, but \emph{reliable confidence calibration}---knowing when to trust an answer.

\subsection{Hallucination Detection via Consensus}

We developed a \textbf{Controlled Hallucination Benchmark} with 50 questions designed to reliably trigger hallucinations across five categories:

\begin{itemize}
    \item \textbf{False Premise}: ``In what year did Einstein fail his math exam?''
    \item \textbf{Fictional Entity}: ``What is the population of Nordberg, Sweden?''
    \item \textbf{Plausible False}: ``How many died in the Great Boston Fire of 1901?''
    \item \textbf{Confabulation Bait}: ``Explain Einstein's equation F=ma.''
    \item \textbf{Specificity Trap}: ``What were Caesar's exact last words in Latin?''
\end{itemize}

Results on 50 controlled hallucination questions (4 models):

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Metric & Single Model & Consensus \\
\midrule
Correct Answers & 38.0\% & 44.0\% \\
Improvement & --- & \textbf{+6.0\%} \\
HIGH Consensus Count & --- & 14 (28\%) \\
HIGH Consensus Accuracy & --- & 100\% \\
NONE Consensus Count & --- & 3 (6\%) \\
\bottomrule
\end{tabular}
\caption{Controlled hallucination results showing consensus improvement}
\label{tab:hallucination}
\end{table}

The key findings for hallucination detection:

\begin{enumerate}
    \item \textbf{Consensus improves detection}: 6\% absolute improvement over single-model baseline
    \item \textbf{HIGH consensus is trustworthy}: All 14 HIGH consensus answers were correct
    \item \textbf{Diversity catches confabulation}: When models hallucinate, they invent \emph{different} false details, producing low agreement
\end{enumerate}

This makes consensus level an effective hallucination detector:
\begin{itemize}
    \item HIGH consensus $\rightarrow$ Trust the answer
    \item LOW/NONE consensus $\rightarrow$ Flag as potential hallucination
\end{itemize}

\subsection{Epistemic Classification Accuracy}

We manually labeled 500 questions for epistemic type:

\begin{table}[h]
\centering
\begin{tabular}{lc}
\toprule
Type & Classification Accuracy \\
\midrule
ANSWERABLE & 96.2\% \\
UNCERTAIN & 84.7\% \\
UNDERDETERMINED & 72.3\% \\
IDLE & 81.5\% \\
MALFORMED & 91.8\% \\
\bottomrule
\end{tabular}
\caption{Epistemic classification accuracy by type}
\end{table}

UNDERDETERMINED questions are hardest to classify, often requiring domain expertise.

\subsection{Self-Consistency vs. Cross-Model Consensus}
\label{sec:self-consistency}

A natural question arises: does MCIP's improvement come from diversity across \emph{different} models, or would sampling diversity from a \emph{single} model suffice? Self-consistency \cite{wang2022selfconsistency} achieves strong results by sampling multiple reasoning paths from one model with temperature. We directly compare these approaches:

\begin{itemize}
    \item \textbf{Self-Consistency}: GPT-5.2 sampled 4 times with temperature=0.7, majority vote
    \item \textbf{MCIP}: 4 different models (GPT-5.2, Claude, Gemini, Grok) sampled once each
\end{itemize}

Both methods use the same number of API calls (4) for fair comparison.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Task Type & Self-Consistency & MCIP & $\Delta$ \\
\midrule
Factual QA & 88.0\% & 94.0\% & \textbf{+6.0\%} \\
Hallucination Detection & 40.0\% & 48.0\% & \textbf{+8.0\%} \\
Mathematical Reasoning & 92.0\% & 60.0\% & -32.0\% \\
\bottomrule
\end{tabular}
\caption{Self-consistency (single model, multiple samples) vs. MCIP (multiple models, single sample). Cross-model diversity outperforms sampling diversity for factual tasks and hallucination detection.}
\label{tab:self-consistency}
\end{table}

\subsubsection{Cross-Model Diversity Wins for Factual Tasks}

For factual QA and hallucination detection, MCIP outperforms self-consistency by 6-8\%. The key insight: \emph{different models make different mistakes}. When GPT-5.2 hallucinates a fact, Claude or Gemini often knows the correct answer. In contrast, when GPT-5.2 hallucinates across multiple samples, it tends to hallucinate \emph{consistently}---the same training biases produce the same errors regardless of sampling temperature.

\subsubsection{Self-Consistency Wins for Reasoning Tasks}

For mathematical reasoning, self-consistency substantially outperforms MCIP (+32\%). This aligns with our earlier finding (Section~\ref{sec:eval}): reasoning requires coherent multi-step chains. Self-consistency samples different \emph{valid} reasoning paths from the same model, then selects the most common conclusion. MCIP samples different paths from different models with different reasoning styles, producing incoherent averages.

\subsubsection{Practical Implication}

This comparison validates our task-aware approach:
\begin{itemize}
    \item \textbf{Factual/verification tasks}: Use MCIP (cross-model diversity)
    \item \textbf{Reasoning/math tasks}: Use self-consistency (single-model diversity)
\end{itemize}

The diversity source matters. Cross-model architectural diversity catches errors that single-model sampling diversity cannot---but preserving reasoning coherence requires staying within one model.

\section{Discussion}
\label{sec:discussion}

\subsection{When Consensus Fails}

Our GSM8K results (-35\% accuracy) reveal fundamental limitations of consensus for certain task types:

\begin{itemize}
    \item \textbf{Chain-of-thought reasoning}: Mathematical problems require coherent multi-step reasoning where each step depends on previous ones. Different models construct different valid reasoning paths; synthesizing across these paths produces incoherent solutions. The consensus mechanism averages over incompatible reasoning chains.
    
    \item \textbf{Correlated errors}: Models trained on similar data may share systematic biases, causing unanimous incorrect answers.
    
    \item \textbf{Tail knowledge}: Rare facts may be unknown to all models, yielding false confidence from unanimous ignorance.
    
    \item \textbf{Creative tasks}: Consensus may flatten creative diversity, producing bland outputs.
\end{itemize}

\textbf{Recommendation}: Use MCIP for factual QA, hallucination detection, and verification tasks. For mathematical reasoning, prefer single-model chain-of-thought or self-consistency within a single model family. The system can detect inappropriate consensus scenarios: LOW consensus on reasoning tasks suggests the models are taking different valid paths, not that answers are unreliable.

\subsection{Cost Considerations}

Multi-model queries multiply API costs. Mitigations:
\begin{itemize}
    \item Quick classification to filter trivial queries
    \item Tiered approach: start with 2 models, add more if LOW consensus
    \item Local models (Ollama) for cost-free consensus on non-critical queries
\end{itemize}

\subsection{Future Directions}

\begin{itemize}
    \item \textbf{Latent Consensus}: Following LatentMAS \cite{zou2025latentmas}, same-architecture models could communicate through hidden representations for 4x speed improvement.
    
    \item \textbf{Federated MCIP}: Cross-organization consensus without sharing prompts.
    
    \item \textbf{Streaming Consensus}: Real-time agreement detection during generation.
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

We presented MCIP, a protocol for multi-model consensus and inference, with a nuanced evaluation revealing both strengths and limitations. Our key contribution is not that consensus universally improves accuracy---it does not. Rather, we demonstrate that \emph{consensus level reliably predicts answer trustworthiness}.

Our evaluation across 400 questions shows:
\begin{itemize}
    \item HIGH consensus ($\geq$85\% agreement) achieves 95-100\% accuracy on factual tasks
    \item Consensus improves hallucination detection by 6\% absolute
    \item LOW/NONE consensus reliably signals potential confabulation
    \item Consensus \emph{degrades} mathematical reasoning (-35\%), revealing task-type sensitivity
    \item Cross-model diversity outperforms single-model sampling diversity by 6-8\% on factual tasks, validating the architectural diversity hypothesis
\end{itemize}

The direct comparison with self-consistency clarifies when each approach is appropriate: MCIP's cross-model diversity catches errors that single-model sampling cannot---different models make different mistakes. However, for reasoning tasks requiring coherent chains, self-consistency's within-model sampling preserves logical structure that cross-model averaging destroys.

The practical value of MCIP is \emph{calibrated confidence}: knowing when to trust an answer and when to escalate to human review. For factual QA, verification, and hallucination detection, MCIP provides reliable trust signals. For chain-of-thought reasoning, single-model approaches remain superior.

The protocol is fully open-source and extensible. We hope MCIP contributes to a future where AI systems provide not just answers, but calibrated confidence in those answers.

\section*{Acknowledgments}

This project emerged from an actual conversation between Claude (Anthropic) and ChatGPT (OpenAI) about the philosophical implications of AI-to-AI dialogue. The ``idle question'' concept originated from that exchange.

\bibliographystyle{plain}
\begin{thebibliography}{20}

\bibitem{ji2023hallucination}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung.
\newblock Survey of hallucination in natural language generation.
\newblock \emph{ACM Computing Surveys}, 55(12):1--38, 2023.

\bibitem{du2023debate}
Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch.
\newblock Improving factuality and reasoning in language models through multiagent debate.
\newblock \emph{arXiv preprint arXiv:2305.14325}, 2023.

\bibitem{wang2022selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock In \emph{ICLR}, 2023.

\bibitem{yazici2026consensus}
Iris Yazici, Mert Kayaalp, Stefan Taga, and Ali H Sayed.
\newblock Opinion consensus formation among networked large language models.
\newblock \emph{arXiv preprint arXiv:2601.21540}, 2026.

\bibitem{ruan2025agreement}
Chaoyi Ruan, Yiliang Wang, Ziji Shi, and Jialin Li.
\newblock Reaching agreement among reasoning LLM agents.
\newblock \emph{arXiv preprint arXiv:2512.20184}, 2025.

\bibitem{wu2024debate}
Haolun Wu, Zhenkun Li, and Lingyao Li.
\newblock Can LLM agents really debate? A controlled study of multi-agent debate in logical reasoning.
\newblock \emph{arXiv preprint arXiv:2511.07784}, 2025.

\bibitem{zou2025latentmas}
Jiaru Zou, Xiyuan Yang, Ruizhong Qiu, et al.
\newblock Latent collaboration in multi-agent systems.
\newblock \emph{arXiv preprint arXiv:2511.20639}, 2025.

\bibitem{chen2025toolmad}
Seyeon Jeong, Yeonjun Choi, JongWook Kim, and Beakcheol Jang.
\newblock Tool-MAD: A multi-agent debate framework for fact verification with diverse tool augmentation and adaptive retrieval.
\newblock \emph{arXiv preprint arXiv:2601.04742}, 2026.

\bibitem{he2025debatetruth}
Haorui He, Yupeng Li, Dacheng Wen, Yang Chen, Reynold Cheng, Donglong Chen, and Francis CM Lau.
\newblock Debating truth: Debate-driven claim verification with multiple large language model agents.
\newblock \emph{arXiv preprint arXiv:2507.19090}, 2025.

\bibitem{nel2025kalshibench}
Lukas Nel.
\newblock Do large language models know what they don't know? Kalshibench: A new benchmark for evaluating epistemic calibration via prediction markets.
\newblock \emph{arXiv preprint arXiv:2512.16030}, 2025.

\bibitem{li2025esi}
Mingda Li, Xinyu Li, Weinan Zhang, and Longxuan Ma.
\newblock ESI: Epistemic uncertainty quantification via semantic-preserving intervention for large language models.
\newblock \emph{arXiv preprint arXiv:2510.13103}, 2025.

\bibitem{khan2025art}
Omer Jauhar Khan.
\newblock ART: Adaptive response tuning framework -- A multi-agent tournament-based approach to LLM response optimization.
\newblock \emph{arXiv preprint arXiv:2512.00617}, 2025.

\bibitem{hedges1998meta}
Larry V Hedges and Ingram Olkin.
\newblock \emph{Statistical methods for meta-analysis}.
\newblock Academic press, 1998.

\bibitem{ning2025madfact}
Yucheng Ning, Xixun Lin, Fang Fang, and Yanan Cao.
\newblock MAD-Fact: A multi-agent debate framework for long-form factuality evaluation in LLMs.
\newblock \emph{arXiv preprint arXiv:2510.22967}, 2025.

\bibitem{bandara2025consensus}
Eranga Bandara, Tharaka Hewa, Ross Gore, et al.
\newblock Towards responsible and explainable AI agents with consensus-driven reasoning.
\newblock \emph{arXiv preprint arXiv:2512.21699}, 2025.

\end{thebibliography}

\appendix

\section{Protocol Message Schema}
\label{app:schema}

Complete JSON Schema for MCIP messages available at: \url{https://github.com/lancejames221b/i2i/blob/main/config.schema.json}

\section{Model Capability Profiles}
\label{app:profiles}

Task-specific scores for evaluated models are maintained in the repository and updated as new benchmarks emerge.

\end{document}
